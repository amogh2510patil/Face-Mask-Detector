{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\prabh\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\tqdm\\auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "c:\\Users\\prabh\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\torchvision\\io\\image.py:13: UserWarning: Failed to load image Python extension: Could not find module 'C:\\Users\\prabh\\AppData\\Local\\Programs\\Python\\Python39\\Lib\\site-packages\\torchvision\\image.pyd' (or one of its dependencies). Try using the full path with constructor syntax.\n",
      "  warn(f\"Failed to load image Python extension: {e}\")\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torchvision import datasets, transforms\n",
    "import cv2\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = transforms.Compose([transforms.Resize(255),\n",
    "                                 transforms.CenterCrop(224),\n",
    "                                 transforms.ToTensor()])\n",
    "\n",
    "dataset = datasets.ImageFolder('D:/amogh/python_projects/ML/Mask_detection/dataset', transform=transform)\n",
    "                                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_size = int(0.8 * len(dataset))\n",
    "test_size = len(dataset) - train_size\n",
    "train_dataset, test_dataset = torch.utils.data.random_split(dataset, [train_size, test_size])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = torch.utils.data.DataLoader(train_dataset, batch_size=100, shuffle=True,num_workers=2,pin_memory=True)\n",
    "test_data = torch.utils.data.DataLoader(test_dataset, batch_size=32, shuffle=False,num_workers=2,pin_memory=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cpu')"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else \"cpu\")\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([32, 3, 224, 224])"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "next(iter(train_data))[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "model=nn.Sequential(nn.Conv2d(3,16,3),nn.ReLU(),nn.MaxPool2d(2,2),nn.Conv2d(16,32,3),nn.ReLU(),nn.MaxPool2d(2,2),nn.Conv2d(32,16,3),nn.ReLU(),nn.MaxPool2d(2,2),nn.Flatten(),nn.Linear(16*26*26,256),nn.ReLU(),nn.BatchNorm1d(256),nn.Dropout(p=0.5),nn.Linear(256,2)).to(device)\n",
    "# model=nn.Sequential(nn.Conv2d(3,16,5),nn.ReLU(),nn.MaxPool2d(4,4),nn.Conv2d(16,8,5),nn.ReLU(),nn.MaxPool2d(4,4),nn.Flatten(),nn.Linear(8*12*12,256),nn.ReLU(),nn.BatchNorm1d(256),nn.Dropout(p=0.5),nn.Linear(256,2))\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "# optimiser = torch.optim.Adam(model.parameters(),lr=0.001,weight_decay=1e-2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "from fastai.optimizer import OptimWrapper\n",
    "from torch import optim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def opt_func(params, **kwargs): return OptimWrapper(torch.optim.Adam(params,lr=0.001,weight_decay=1e-2))\n",
    "from functools import partial\n",
    "opt_func = partial(OptimWrapper, opt=torch.optim.Adam)\n",
    "\n",
    "from fastai.data.core import DataLoaders\n",
    "dls = DataLoaders(train_data, test_data)\n",
    "\n",
    "from fastai.learner import Learner\n",
    "from fastai.callback.progress import ProgressCallback\n",
    "\n",
    "# from fastai.callback.data import CudaCallback\n",
    "\n",
    "learn = Learner(dls, model, loss_func=criterion, opt_func=opt_func)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "<style>\n",
       "    /* Turns off some styling */\n",
       "    progress {\n",
       "        /* gets rid of default border in Firefox and Opera. */\n",
       "        border: none;\n",
       "        /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
       "        background-size: auto;\n",
       "    }\n",
       "    .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {\n",
       "        background: #F44336;\n",
       "    }\n",
       "</style>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>epoch</th>\n",
       "      <th>train_loss</th>\n",
       "      <th>valid_loss</th>\n",
       "      <th>time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>0.298506</td>\n",
       "      <td>0.236451</td>\n",
       "      <td>00:30</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.230851</td>\n",
       "      <td>0.201727</td>\n",
       "      <td>00:30</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.224184</td>\n",
       "      <td>0.223974</td>\n",
       "      <td>00:29</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.227519</td>\n",
       "      <td>0.403179</td>\n",
       "      <td>00:29</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.194858</td>\n",
       "      <td>0.202201</td>\n",
       "      <td>00:29</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.193322</td>\n",
       "      <td>0.191457</td>\n",
       "      <td>00:29</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>0.173350</td>\n",
       "      <td>0.180735</td>\n",
       "      <td>00:29</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>0.177377</td>\n",
       "      <td>0.174224</td>\n",
       "      <td>00:29</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>0.152518</td>\n",
       "      <td>0.200904</td>\n",
       "      <td>00:29</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>0.149348</td>\n",
       "      <td>0.232862</td>\n",
       "      <td>00:29</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>0.131829</td>\n",
       "      <td>0.160545</td>\n",
       "      <td>00:30</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11</td>\n",
       "      <td>0.114513</td>\n",
       "      <td>0.190809</td>\n",
       "      <td>00:30</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12</td>\n",
       "      <td>0.113138</td>\n",
       "      <td>0.156504</td>\n",
       "      <td>00:29</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13</td>\n",
       "      <td>0.100399</td>\n",
       "      <td>0.259837</td>\n",
       "      <td>00:29</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14</td>\n",
       "      <td>0.102554</td>\n",
       "      <td>0.226991</td>\n",
       "      <td>00:30</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15</td>\n",
       "      <td>0.081225</td>\n",
       "      <td>0.149248</td>\n",
       "      <td>00:29</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16</td>\n",
       "      <td>0.104344</td>\n",
       "      <td>0.163298</td>\n",
       "      <td>00:31</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17</td>\n",
       "      <td>0.087561</td>\n",
       "      <td>0.208317</td>\n",
       "      <td>00:30</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18</td>\n",
       "      <td>0.079933</td>\n",
       "      <td>0.177097</td>\n",
       "      <td>00:29</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19</td>\n",
       "      <td>0.078308</td>\n",
       "      <td>0.178244</td>\n",
       "      <td>00:29</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "learn.fit(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "            Conv2d-1         [-1, 16, 222, 222]             448\n",
      "              ReLU-2         [-1, 16, 222, 222]               0\n",
      "         MaxPool2d-3         [-1, 16, 111, 111]               0\n",
      "            Conv2d-4         [-1, 32, 109, 109]           4,640\n",
      "              ReLU-5         [-1, 32, 109, 109]               0\n",
      "         MaxPool2d-6           [-1, 32, 54, 54]               0\n",
      "            Conv2d-7           [-1, 16, 52, 52]           4,624\n",
      "              ReLU-8           [-1, 16, 52, 52]               0\n",
      "         MaxPool2d-9           [-1, 16, 26, 26]               0\n",
      "          Flatten-10                [-1, 10816]               0\n",
      "           Linear-11                  [-1, 256]       2,769,152\n",
      "             ReLU-12                  [-1, 256]               0\n",
      "      BatchNorm1d-13                  [-1, 256]             512\n",
      "          Dropout-14                  [-1, 256]               0\n",
      "           Linear-15                    [-1, 2]             514\n",
      "================================================================\n",
      "Total params: 2,779,890\n",
      "Trainable params: 2,779,890\n",
      "Non-trainable params: 0\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 0.57\n",
      "Forward/backward pass size (MB): 20.88\n",
      "Params size (MB): 10.60\n",
      "Estimated Total Size (MB): 32.06\n",
      "----------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "from torchsummary import summary\n",
    "summary(model,(3,224,224))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "# epochs=2\n",
    "# for epoch in range(epochs):\n",
    "#     cor=0\n",
    "#     for (imgs,labels) in train_data:\n",
    "#         imgs = imgs.to(device)\n",
    "#         labels=labels.to(device)\n",
    "#         output=model(imgs)\n",
    "#         loss=criterion(output,labels)\n",
    "#         for param in model.parameters():\n",
    "#             param.grad=None\n",
    "#         loss.backward()\n",
    "#         optimiser.step()\n",
    "\n",
    "        \n",
    "#         _,predicted = torch.max(output.data,1)\n",
    "#         cor+=(predicted == labels).sum().item()\n",
    "#         # acc=sum(output.round()==y_train)/len(y_train)\n",
    "#     print(epoch,cor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "FILE='model_pth'\n",
    "torch.save(model.state_dict(),FILE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "49672192"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "import matplotlib.pyplot as plt\n",
    "torch.cuda.memory_allocated(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "94.74969474969474\n"
     ]
    }
   ],
   "source": [
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    num_img=0\n",
    "    cor=0\n",
    "    for (imgs,labels) in test_data:\n",
    "        imgs = imgs.to(device)\n",
    "        labels=labels.to(device)\n",
    "        output=model(imgs)\n",
    "        _,predicted = torch.max(output.data,1)\n",
    "        cor+=(predicted == labels).sum().item()\n",
    "        num_img+=predicted.shape[0]\n",
    "        \n",
    "        # image=imgs[0]\n",
    "        # plt.imshow(transforms.ToPILImage()(image))\n",
    "        # plt.show()\n",
    "        # print(predicted[0].item(),labels[0].item())\n",
    "        # cv2.imshow(\"\",np.array(image)*255)\n",
    "        # cv2.waitKey(0)\n",
    "\n",
    "    print(cor/num_img*100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading model...................\n"
     ]
    }
   ],
   "source": [
    "#defining prototext and caffemodel paths\n",
    "caffeModel = \"D:/amogh/python_projects/ML/Face_Detection/Face-detection-with-OpenCV-and-deep-learning-master/models/res10_300x300_ssd_iter_140000.caffemodel\"\n",
    "prototextPath = \"D:/amogh/python_projects/ML/Face_Detection/Face-detection-with-OpenCV-and-deep-learning-master/models/deploy.prototxt.txt\"\n",
    "\n",
    "#Load Model\n",
    "print(\"Loading model...................\")\n",
    "net = cv2.dnn.readNetFromCaffe(prototextPath,caffeModel)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "dic={0:\"MASK\",1:\"NO MASK\"}\n",
    "colour_dic={0:(0,255,0),1:(0,0,255)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] starting video stream...\n"
     ]
    }
   ],
   "source": [
    "# initialize the video stream to get the video frames\n",
    "print(\"[INFO] starting video stream...\")\n",
    "camera=cv2.VideoCapture(0)\n",
    "\n",
    "while True :\n",
    "    \n",
    "    _,image=camera.read()\n",
    "    \n",
    "    (h, w) = image.shape[:2]\n",
    "    try:\n",
    "        blob = cv2.dnn.blobFromImage(cv2.resize(image, (300, 300)), 1.0, (300, 300), (104.0, 177.0, 123.0))\n",
    "\n",
    "        net.setInput(blob)\n",
    "        detections = net.forward()\n",
    "\n",
    "        # Identify each face\n",
    "        for i in range(0, detections.shape[2]):\n",
    "            box = detections[0, 0, i, 3:7] * np.array([w, h, w, h])\n",
    "            (startX, startY, endX, endY) = box.astype(\"int\")\n",
    "\n",
    "            confidence = detections[0, 0, i, 2]\n",
    "\n",
    "            # If confidence > 0.5, save it as a separate file\n",
    "            if (confidence > 0.5):\n",
    "                frame = image[startY-50:endY+50, startX-50:endX+50]\n",
    "                frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "            # show the output frame\n",
    "                test_img = np.moveaxis(frame,2,0)\n",
    "            \n",
    "                transform = transforms.Compose([transforms.Resize(255),\n",
    "                                            transforms.CenterCrop(224),\n",
    "                                            ])\n",
    "                test_img=transform(torch.FloatTensor(test_img))/255\n",
    "                test_imgs = torch.utils.data.DataLoader([test_img], batch_size=1, shuffle=True,num_workers=0)\n",
    "                \n",
    "               \n",
    "                with torch.no_grad():\n",
    "                    img=next(iter(test_imgs))\n",
    "                    # for imgs in test_imgs:\n",
    "                        # pass\n",
    "                        # imgs = imgs.to(device)\n",
    "                    output=model(img.to(device))\n",
    "                    _,predicted = torch.max(output.data,1)\n",
    "                    # image=imgs[0]\n",
    "                    # plt.imshow(transforms.ToPILImage()(image))\n",
    "                    # plt.show()\n",
    "                    # print(predicted)\n",
    "                \n",
    "                text = \"{:.2f}%\".format(confidence * 100)\n",
    "                y = startY - 10 if startY - 10 > 10 else startY + 10\n",
    "                \n",
    "                cv2.rectangle(image, (startX, startY), (endX, endY),\n",
    "                            colour_dic[predicted.item()], 2)\n",
    "                cv2.putText(image, dic[predicted.item()]+'    '+str(text), (startX, y),\n",
    "                            cv2.FONT_HERSHEY_SIMPLEX, 0.45, colour_dic[predicted.item()], 2)\n",
    "    except:pass\n",
    "    cv2.imshow(\"Frame\", image)\n",
    "    key = cv2.waitKey(1) & 0xFF\n",
    "\n",
    "    # if the `q` key was pressed, break from the loop\n",
    "    if key == ord(\"q\"):\n",
    "        break\n",
    "\n",
    "# do a bit of cleanup\n",
    "camera.release()\n",
    "cv2.destroyAllWindows()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.5 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "9ba410bc5b297c41942d0f23bd30a3f98c8d0ce10a2b1477471cb9f64ae06750"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
